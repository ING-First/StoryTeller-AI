#!/usr/bin/env python3
"""
íŒŒì¸íŠœë‹ëœ ë™í™” í‰ê°€ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

def load_model():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    print("Loading model...")
    
    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    base_model = AutoModelForCausalLM.from_pretrained(
        "skt/A.X-4.0-Light",
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # LoRA ì–´ëŒ‘í„° ë¡œë“œ
    model = PeftModel.from_pretrained(base_model, ".")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(".", trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer

def evaluate_fairy_tale(model, tokenizer, story):
    """ë™í™” í‰ê°€"""
    prompt = [
        {
            "role": "system", 
            "content": "ë‹¹ì‹ ì€ ë™í™”ë¥¼ í‰ê°€í•˜ëŠ” AIì…ë‹ˆë‹¤. ë‹¤ìŒ ë™í™”ì— ëŒ€í•´ ì•„ë˜ 8ê°€ì§€ ê¸°ì¤€ì— ë”°ë¼ ê°ê° 1~5ì ìœ¼ë¡œ ì ìˆ˜ë§Œ ë§¤ê²¨ì£¼ì„¸ìš”."
        },
        {
            "role": "user", 
            "content": f"ì´ ë™í™”ë¥¼ í‰ê°€í•´ì¤˜\n\n### ë™í™”:\n{story}"
        }
    ]
    
    text = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(text, return_tensors="pt", max_length=1024, truncation=True)
    
    if torch.cuda.is_available():
        inputs = inputs.to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=False,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    result = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    return result

if __name__ == "__main__":
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model()
    
    # í…ŒìŠ¤íŠ¸ ë™í™”
    test_story = """ì‘ì€ ë§ˆì„ì—ëŠ” ê²¨ìš¸ì´ ì˜¤ë©´ ëª¨ë“  ê²ƒì´ ì–¼ìŒìœ¼ë¡œ ë®ì˜€ì–´ìš”. ì•„ì´ë“¤ì€ í•˜ì–€ ì„¸ìƒì—ì„œ ëˆˆì‹¸ì›€ì„ í•˜ë©° í–‰ë³µí•œ ì‹œê°„ì„ ë³´ëƒˆì–´ìš”. ê·¸ëŸ°ë° ì–´ëŠ ë‚ , ë°”ëŒì´ ì´ìƒí•˜ê²Œë„ ê·€ì—½ê²Œ íœ™ ë¶ˆì–´ì˜¤ë”ë‹ˆ ì‘ì€ ë§ˆë²• ì–‘íƒ„ìë¥¼ ê°€ì ¸ì™”ì–´ìš”."""
    
    # í‰ê°€ ìˆ˜í–‰
    print("ğŸ§šâ€â™€ï¸ ë™í™” í‰ê°€ í…ŒìŠ¤íŠ¸")
    print(f"ë™í™”: {test_story}")
    print("\ní‰ê°€ ê²°ê³¼:")
    
    result = evaluate_fairy_tale(model, tokenizer, test_story)
    print(result)
