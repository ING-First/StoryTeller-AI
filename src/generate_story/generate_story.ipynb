{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5TZ8bEkKDFd",
        "outputId": "951f69f3-310a-464e-cedf-f2e66bc70a88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XNtbYZSKKBb",
        "outputId": "06420a1b-af82-48cf-b490-8df4e540b8ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -y\n"
          ]
        }
      ],
      "source": [
        "!pip install -y torch torchvision torchaudio bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy3-KcoeNYMz",
        "outputId": "9a49db23-afd3-4e49-adfe-9e9bc2026ac9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96rZIji-9ZCp"
      },
      "source": [
        "# run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AFYM4C6U80qi"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade fastapi uvicorn nest_asyncio transformers huggingface_hub pycloudflared pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444,
          "referenced_widgets": [
            "1226f8c00dd244d7901ec4182ecf2484",
            "1fecd95216d343769adb1fa41349c3c3",
            "08bb4c8c0824429d818681def026fedd",
            "3c3e441c019f401b894b38044776e3d4",
            "57acd1639b0d4fad97f8b734978e7edc",
            "10e62def10134146af20a3f5eab93229",
            "1b8de40e25744e4595b929162ea6ab6d",
            "64c396a91310465ea0e6c097d534142f",
            "f103dd8a5cf1403e919e3e37b4cef01d",
            "078c5695779d45dc98dcf384b41e554f",
            "2b62d9a6766a4e70b92d5b1d6a2e3845"
          ]
        },
        "id": "apyrJRsy9fbK",
        "outputId": "d4449446-8332-40ed-cebc-52fd353b3a0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4115756812.py:346: DeprecationWarning: \n",
            "        on_event is deprecated, use lifespan event handlers instead.\n",
            "\n",
            "        Read more about it in the\n",
            "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
            "        \n",
            "  @app.on_event(\"startup\")\n",
            "INFO:     Started server process [51079]\n",
            "INFO:     Waiting for application startup.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF_TOKEN loaded: YES\n",
            "Loading merged model from HF Hub / local path...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1226f8c00dd244d7901ec4182ecf2484",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded in 10.24s\n",
            "[GPU] Î°úÎìú ÏßÅÌõÑ | ÌòÑÏû¨ Ìï†Îãπ: 13849.14 MB | ÌòÑÏû¨ ÏòàÏïΩ: 13870.00 MB | ÌîºÌÅ¨ Ìï†Îãπ: 13849.14 MB\n",
            "[OK] Uvicorn running on http://127.0.0.1:8001\n",
            " * Running on https://medium-exp-dozens-resource.trycloudflare.com\n",
            " * Traffic stats available on http://127.0.0.1:20241/metrics\n",
            "üåê Cloudflare URL: Urls(tunnel='https://medium-exp-dozens-resource.trycloudflare.com', metrics='http://127.0.0.1:20241/metrics', process=<Popen: returncode: None args: ['/usr/local/lib/python3.11/dist-packages/pyc...>)\n",
            "  - GET : Urls(tunnel='https://medium-exp-dozens-resource.trycloudflare.com', metrics='http://127.0.0.1:20241/metrics', process=<Popen: returncode: None args: ['/usr/local/lib/python3.11/dist-packages/pyc...>)/health\n",
            "  - POST: Urls(tunnel='https://medium-exp-dozens-resource.trycloudflare.com', metrics='http://127.0.0.1:20241/metrics', process=<Popen: returncode: None args: ['/usr/local/lib/python3.11/dist-packages/pyc...>)/generate\n"
          ]
        }
      ],
      "source": [
        "import os, re\n",
        "import time\n",
        "import torch\n",
        "import threading, socket, contextlib\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Tuple\n",
        "\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "from huggingface_hub import HfFolder\n",
        "\n",
        "\n",
        "def load_hf_token_from_colab_secret(name=\"HF_TOKEN\"):\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        tok = userdata.get(name)\n",
        "        if tok and tok.strip():\n",
        "            return tok.strip()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    tok = os.getenv(name, \"\").strip()\n",
        "    return tok or None\n",
        "\n",
        "HF_TOKEN = load_hf_token_from_colab_secret(\"HF_TOKEN\")\n",
        "print(\"HF_TOKEN loaded:\", \"YES\" if HF_TOKEN else \"NO\")\n",
        "\n",
        "# gpu check\n",
        "def _fmt_mb(b: int) -> str:\n",
        "    return f\"{b/(1024**2):.2f} MB\"\n",
        "\n",
        "def _gpu_stats(tag: str) -> str:\n",
        "    if not torch.cuda.is_available():\n",
        "        return f\"[GPU] {tag}: CUDA ÎØ∏ÏÇ¨Ïö©\"\n",
        "    torch.cuda.synchronize()\n",
        "    a = torch.cuda.memory_allocated()\n",
        "    r = torch.cuda.memory_reserved()\n",
        "    p = torch.cuda.max_memory_allocated()\n",
        "    return f\"[GPU] {tag} | ÌòÑÏû¨ Ìï†Îãπ: {_fmt_mb(a)} | ÌòÑÏû¨ ÏòàÏïΩ: {_fmt_mb(r)} | ÌîºÌÅ¨ Ìï†Îãπ: {_fmt_mb(p)}\"\n",
        "\n",
        "def _select_dtype_and_device_map():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.float16, \"auto\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return torch.float16, {\"\": \"mps\"}\n",
        "    return torch.float32, {\"\": \"cpu\"}\n",
        "\n",
        "# model load & generate\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    repo_or_path: str = \"kkuriyoon/QLoRA-ax4-StoryTeller\"\n",
        "    hf_token: Optional[str] = HF_TOKEN\n",
        "    trust_remote_code: bool = True\n",
        "    low_cpu_mem_usage: bool = False\n",
        "    use_safetensors: bool = True\n",
        "\n",
        "@dataclass\n",
        "class GenConfig:\n",
        "    max_new_tokens: int = 280\n",
        "    temperature: float = 0.8\n",
        "    top_p: float = 0.9\n",
        "    do_sample: bool = True\n",
        "    repetition_penalty: float = 1.05\n",
        "    def to_hf(self) -> GenerationConfig:\n",
        "        return GenerationConfig(\n",
        "            max_new_tokens=self.max_new_tokens,\n",
        "            temperature=self.temperature,\n",
        "            top_p=self.top_p,\n",
        "            do_sample=self.do_sample,\n",
        "            repetition_penalty=self.repetition_penalty,\n",
        "        )\n",
        "\n",
        "@dataclass\n",
        "class PromptConfig:\n",
        "    length_hint: str = \"6~8Î¨∏Ïû•\"\n",
        "    reading_level: str = \"Ìï¥Îãπ ÎÇòÏù¥ ÎòêÎûòÍ∞Ä Ïà†Ïà† ÏùΩÏùÑ Ïàò ÏûàÎäî ÎÇúÏù¥ÎèÑ\"\n",
        "    safety: str = \"Ìè≠Î†•/Í≥µÌè¨/ÌòêÏò§/Ïó∞Î†πÎ∂àÍ∞Ä ÏöîÏÜå Í∏àÏßÄ\"\n",
        "    style_override: Optional[str] = None\n",
        "    genre_guides: Dict[str, str] = field(default_factory=lambda: {\n",
        "        \"ÎèôÌôî\": \"Îî∞ÎúªÌïòÍ≥† Ìè¨Í∑ºÌïú ÌÜ§, ÏùºÏÉÅÏ†Å Í∞àÎì±Í≥º ÏûëÏùÄ Ìï¥Í≤∞, ÏùòÏÑ±Ïñ¥/ÏùòÌÉúÏñ¥ ÏÜåÎüâ\",\n",
        "        \"Î™®Ìóò\": \"Í≤ΩÏæåÌïú ÏßÑÌñâ, Î™©Ìëú-Ïû•Ïï†-ÏÑ±Ïû•Ïùò 3Îßâ, Í≥µÍ∞Ñ Ïù¥ÎèôÍ≥º ÏûëÏùÄ ÌÄòÏä§Ìä∏\",\n",
        "        \"ÎØ∏Ïä§ÌÑ∞Î¶¨\": \"Î∂ÄÎìúÎü¨Ïö¥ Ìò∏Í∏∞Ïã¨ Ïú†Î∞ú, ÏúÑÌóò ÏµúÏÜåÌôî, Îã®ÏÑú-Ï∂îÎ°†-Ìï¥Í≤∞Ïùò ÌùêÎ¶Ñ\",\n",
        "        \"ÌåêÌÉÄÏßÄ\": \"ÏÉÅÏÉÅÎ†• Í∞ÄÎìùÌïú ÏÑ∏Í≥ÑÍ¥Ä, ÎßàÎ≤ï/ÏÉÅÏßïÏùÑ ÏùÄÏú†Ï†ÅÏúºÎ°ú ÏÇ¨Ïö©\",\n",
        "        \"SF\": \"ÎØ∏Îûò/Í≥ºÌïô ÏöîÏÜåÎ•º ÏâΩÍ≥† ÏïàÏ†ÑÌïòÍ≤å ÏÑ§Î™Ö, Í∏∞Ïà†ÏùÄ ÏπúÍ∑ºÌïú ÎèÑÍµ¨Ï≤òÎüº\",\n",
        "        \"ÏùºÏÉÅ\": \"ÏπúÍµ¨/Í∞ÄÏ°±/ÌïôÍµê/ÎèôÎÑ§ Îì± Í≥µÍ∞ê Ìè¨Ïù∏Ìä∏ Ï§ëÏã¨Ïùò ÏÜåÏÜåÌïú ÏÇ¨Í±¥\",\n",
        "        \"ÎèôÏãú\": \"Î¶¨Îì¨/Î∞òÎ≥µ/Ïù¥ÎØ∏ÏßÄÎ•º ÏÇ¥Î¶∞ Ïö¥Ïú®, ÏßßÏùÄ ÌñâÍ≥º Î™ÖÎ£åÌïú Î©îÏãúÏßÄ\",\n",
        "    })\n",
        "\n",
        "# main\n",
        "class StoryTeller:\n",
        "    def __init__(self, model_cfg: ModelConfig, gen_cfg: Optional[GenConfig] = None, prompt_cfg: Optional[PromptConfig] = None):\n",
        "        self.model_cfg = model_cfg\n",
        "        self.gen_cfg = gen_cfg or GenConfig()\n",
        "        self.prompt_cfg = prompt_cfg or PromptConfig()\n",
        "        self.tokenizer: Optional[AutoTokenizer] = None\n",
        "        self.model: Optional[AutoModelForCausalLM] = None\n",
        "        self.load_sec: Optional[float] = None\n",
        "\n",
        "    def load(self) -> Tuple[AutoTokenizer, AutoModelForCausalLM, float]:\n",
        "        print(\"Loading merged model from HF Hub / local path...\")\n",
        "        t0 = time.time()\n",
        "\n",
        "        torch_dtype, device_map = _select_dtype_and_device_map()\n",
        "\n",
        "        # repo Íµ¨Î∂Ñ\n",
        "        token_kw = {\"token\": self.model_cfg.hf_token} if self.model_cfg.hf_token else {}\n",
        "\n",
        "        tok = AutoTokenizer.from_pretrained(\n",
        "            self.model_cfg.repo_or_path,\n",
        "            use_fast=True,\n",
        "            trust_remote_code=self.model_cfg.trust_remote_code,\n",
        "            **token_kw\n",
        "        )\n",
        "        if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
        "            tok.pad_token = tok.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_cfg.repo_or_path,\n",
        "            torch_dtype=torch_dtype,\n",
        "            device_map=device_map,\n",
        "            trust_remote_code=self.model_cfg.trust_remote_code,\n",
        "            low_cpu_mem_usage=self.model_cfg.low_cpu_mem_usage,\n",
        "            use_safetensors=True,\n",
        "            **token_kw\n",
        "        )\n",
        "        model.config.use_cache = True\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.backends.cuda.matmul.allow_tf32 = True\n",
        "            torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "        self.tokenizer = tok\n",
        "        self.model = model\n",
        "        self.load_sec = time.time() - t0\n",
        "\n",
        "        print(f\"Loaded in {self.load_sec:.2f}s\")\n",
        "        print(_gpu_stats(\"Î°úÎìú ÏßÅÌõÑ\"))\n",
        "        return tok, model, self.load_sec\n",
        "\n",
        "    def build_prompt(self, name: str, age: int, genre: str) -> str:\n",
        "        pc = self.prompt_cfg\n",
        "        guide = pc.genre_guides.get(genre, \"Ïû•Î•¥Ï†Å Í¥ÄÏäµÏùÑ Ïú†ÏïÑ ÏπúÌôîÏ†ÅÏúºÎ°ú ÏàúÌôîÌïòÏó¨ Î∞òÏòÅ\")\n",
        "        if pc.style_override:\n",
        "            guide = pc.style_override\n",
        "\n",
        "        return f\"\"\"ÎãπÏã†ÏùÄ ÏïÑÎèô Î¨∏Ìïô ÏûëÍ∞ÄÏù¥Ïûê Ïñ∏Ïñ¥Î∞úÎã¨ ÏΩîÏπòÏûÖÎãàÎã§. ÏïÑÎûò Ï†ïÎ≥¥Î•º Î∞òÏòÅÌïòÏó¨ {age}ÏÇ¥ ÏïÑÏù¥ '{name}'ÏóêÍ≤å Îî± ÎßûÎäî {genre} Ïû•Î•¥ Ïù¥ÏïºÍ∏∞(ÌïúÍµ≠Ïñ¥)Î•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî.\n",
        "\n",
        "[ÎèÖÏûê Ï†ïÎ≥¥]\n",
        "- Ïù¥Î¶Ñ: {name}\n",
        "- ÎÇòÏù¥: {age}ÏÑ∏\n",
        "- ÏùΩÍ∏∞ ÎÇúÏù¥ÎèÑ: {pc.reading_level}\n",
        "\n",
        "[Ïû•Î•¥ Í∞ÄÏù¥Îìú]\n",
        "- {guide}\n",
        "\n",
        "[Ïä§ÌÜ†Î¶¨ Íµ¨ÏÑ±(Ï†úÎ°úÏÉ∑ ÏßÄÏãú)]\n",
        "1) Ï≤´ Î¨∏Ïû•ÏùÄ ÏûêÏó∞Ïä§ÎüΩÍ≤å ÏÉÅÌô©/Ïû•Î©¥/Í∞êÏ†ï Ï§ë ÌïòÎÇòÎ°ú ÏãúÏûëÌï©ÎãàÎã§. ÌäπÏ†ï Í≥†Ï†ï Î¨∏Íµ¨Î°ú ÏãúÏûëÌïòÏßÄ ÎßàÏÑ∏Ïöî.\n",
        "2) {name}Ïù¥(Í∞Ä) Í≤™Îäî ÏûëÏùÄ Ïñ¥Î†§ÏõÄ ‚Üí ÏãúÎèÑ/ÎèÑÏõÄ ‚Üí Ïä§Ïä§Î°ú(ÎòêÎäî ÏπúÍµ¨ÏôÄ Ìï®Íªò) Ìï¥Í≤∞.\n",
        "3) Ïû•Î©¥ Ï†ÑÌôòÏùÄ Í≥ºÎèÑÌïòÏßÄ ÏïäÍ≤å 2~3Ìöå Ïù¥ÎÇ¥Î°ú.\n",
        "4) ÎåÄÏÇ¨Îäî 2~4Í≥≥ÏóêÎßå ÏûêÏó∞Ïä§ÎüΩÍ≤å ÏÑûÎêò, Í≥ºÎèÑÌïú Í∞êÌÉÑÏÇ¨/Î∞òÎ≥µÏùÄ ÏßÄÏñë.\n",
        "5) ÎπÑÏú†/ÏÉÅÏßïÏùÄ ÎÇòÏù¥Ïóê ÎßûÍ≤å Ïâ¨Ïö¥ Îã®Ïñ¥Î°ú.\n",
        "\n",
        "[Í∏∏Ïù¥]\n",
        "- Î¨∏Ïû• Ïàò: {pc.length_hint}\n",
        "\n",
        "[ÏïàÏ†Ñ/Ïú§Î¶¨]\n",
        "- {pc.safety}\n",
        "- Ïã§Ï†ú Ïù∏Î¨º¬∑Î∏åÎûúÎìú¬∑Ï†ïÏπòÏ†Å ÏÇ¨Ïïà Ïñ∏Í∏â Í∏àÏßÄ.\n",
        "- ÌëúÏ†à Í∏àÏßÄ, Ï†ÑÍ∞úÎäî ÏÉàÎ°≠Í≤å Íµ¨ÏÑ±.\n",
        "\n",
        "[Ï∂úÎ†• ÌòïÏãù(Ï§ëÏöî)]\n",
        "- Ïù¥ÏïºÍ∏∞ Î≥∏Î¨∏\n",
        "- Í≥µÎ∞± Ìïú Ï§Ñ\n",
        "- Ï†úÎ™©: (Ïù¥ÏïºÍ∏∞Ïóê Ïñ¥Ïö∏Î¶¨Îäî ÏßßÍ≥† Ïù∏ÏÉÅÏ†ÅÏù∏ Ï±Ö Ï†úÎ™©)\n",
        "\n",
        "Ïù¥Ï†ú '{genre}' Ïû•Î•¥Ïùò Ïù¥ÏïºÍ∏∞Î•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî.\"\"\"\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(self, prompt: str) -> Dict[str, object]:\n",
        "        assert self.tokenizer is not None and self.model is not None, \"Î®ºÏ†Ä load()Î•º Ìò∏Ï∂úÌï¥ Î™®Îç∏ÏùÑ Î°úÎìúÌïòÏÑ∏Ïöî.\"\n",
        "        tok = self.tokenizer\n",
        "        model = self.model\n",
        "\n",
        "        inputs = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        # warmup\n",
        "        _ = model.generate(**{k: v[:, :4] for k, v in inputs.items()}, max_new_tokens=1, do_sample=False)\n",
        "\n",
        "        cfg = self.gen_cfg.to_hf()\n",
        "        t0 = time.time()\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=cfg.max_new_tokens,\n",
        "            temperature=cfg.temperature,\n",
        "            top_p=cfg.top_p,\n",
        "            do_sample=cfg.do_sample,\n",
        "            repetition_penalty=cfg.repetition_penalty,\n",
        "            eos_token_id=tok.eos_token_id,\n",
        "            pad_token_id=tok.pad_token_id,\n",
        "            use_cache=True,\n",
        "        )\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        gen_sec = time.time() - t0\n",
        "\n",
        "        text = tok.decode(out[0], skip_special_tokens=True)\n",
        "        if text.startswith(prompt):\n",
        "            text = text[len(prompt):].lstrip()\n",
        "\n",
        "        new_tokens = out[0].shape[0] - inputs[\"input_ids\"].shape[1]\n",
        "        tokps = new_tokens / max(gen_sec, 1e-6)\n",
        "\n",
        "        gpu_summary = None\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_summary = {\n",
        "                \"alloc_now\": _fmt_mb(torch.cuda.memory_allocated()),\n",
        "                \"reserved_now\": _fmt_mb(torch.cuda.memory_reserved()),\n",
        "                \"peak_alloc\": _fmt_mb(torch.cuda.max_memory_allocated()),\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            \"text\": text,\n",
        "            \"gen_sec\": gen_sec,\n",
        "            \"new_tokens\": new_tokens,\n",
        "            \"tokps\": tokps,\n",
        "            \"gpu\": gpu_summary,\n",
        "        }\n",
        "\n",
        "# title, content\n",
        "def _split_title_content(text: str) -> Tuple[str, str]:\n",
        "    s = text.strip()\n",
        "\n",
        "    # title Ï∂îÏ∂ú\n",
        "    titles = re.findall(r\"Ï†úÎ™©\\s*:\\s*(.+)\", s)\n",
        "    title = titles[0].strip() if titles else \"Ï†úÎ™© ÏóÜÏùå\"\n",
        "\n",
        "    # title ÌõÑÏ≤òÎ¶¨\n",
        "    title = re.sub(r\"\\*\\*\", \"\", title)\n",
        "    title = title.replace(\"\\\\\", \" \")\n",
        "    title = title.replace(\"\\n\", \" \")\n",
        "    title = re.sub(r\"\\s+\", \" \", title).strip()\n",
        "\n",
        "    # content ÌõÑÏ≤òÎ¶¨\n",
        "    content = re.sub(r\"Ï†úÎ™©\\s*:\\s*.+\", \"\", s)\n",
        "    content = re.sub(r\"-{3,}\", \" \", content)\n",
        "    content = re.sub(r\"#\", \" \", content)\n",
        "    content = re.sub(r\"\\*\\*\", \"\", content)\n",
        "    content = content.replace(\"\\\\\", \"\")\n",
        "    content = content.replace(\"\\n\", \" \")\n",
        "    content = re.sub(r\"\\s+\", \" \", content).strip()\n",
        "\n",
        "    return title, content\n",
        "\n",
        "\n",
        "def _split_sentences_kor(s: str):\n",
        "    s = re.sub(r\"\\s+\", \" \", s.strip())\n",
        "    # 'Îã§.', 'Ïöî.' ÎòêÎäî ÏùºÎ∞ò Î¨∏Ïû• Î∂ÄÌò∏Î°ú ÎÅùÎÇòÎäî Íµ¨Í∞ÑÏùÑ Î¨∏Ïû•ÏúºÎ°ú Í∞ÑÏ£º\n",
        "    pat = re.compile(r'.*?(?:Îã§\\.|Ïöî\\.|[.!?‚Ä¶])')\n",
        "    sentences = pat.findall(s)\n",
        "    tail = s[sum(len(x) for x in sentences):].strip()\n",
        "    if tail:\n",
        "        sentences.append(tail)\n",
        "    # Í≥µÎ∞±/Ïû°Î¨∏ Ï†úÍ±∞\n",
        "    sentences = [x.strip() for x in sentences if x.strip()]\n",
        "    return sentences\n",
        "\n",
        "\n",
        "\n",
        "def _paginate_content(content: str, sentences_per_page: int = 3, max_chars: int = 600):\n",
        "    sents = _split_sentences_kor(content)\n",
        "    pages = []\n",
        "    buf = []\n",
        "    for i, sent in enumerate(sents, 1):\n",
        "        buf.append(sent)\n",
        "        if (i % sentences_per_page) == 0:\n",
        "            page = \" \".join(buf).strip()\n",
        "            pages.append(page)\n",
        "            buf = []\n",
        "    if buf:\n",
        "        pages.append(\" \".join(buf).strip())\n",
        "\n",
        "    fixed = []\n",
        "    for p in pages:\n",
        "        if len(p) <= max_chars:\n",
        "            fixed.append(p)\n",
        "        else:\n",
        "            chunk = []\n",
        "            cur = 0\n",
        "            words = p.split(\" \")\n",
        "            for w in words:\n",
        "                if cur + len(w) + (1 if chunk else 0) > max_chars:\n",
        "                    fixed.append(\" \".join(chunk))\n",
        "                    chunk = [w]\n",
        "                    cur = len(w)\n",
        "                else:\n",
        "                    if chunk:\n",
        "                        chunk.append(w); cur += len(w) + 1\n",
        "                    else:\n",
        "                        chunk = [w]; cur = len(w)\n",
        "            if chunk:\n",
        "                fixed.append(\" \".join(chunk))\n",
        "    return fixed\n",
        "\n",
        "\n",
        "# fastapi\n",
        "app = FastAPI(title=\"StoryTeller API\", version=\"1.0.0\")\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "STORY: Optional[StoryTeller] = None\n",
        "\n",
        "class GenerateRequest(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    genre: str\n",
        "    max_new_tokens: Optional[int] = None\n",
        "    temperature: Optional[float] = None\n",
        "    top_p: Optional[float] = None\n",
        "    repetition_penalty: Optional[float] = None\n",
        "    style_override: Optional[str] = None\n",
        "    length_hint: Optional[str] = None\n",
        "\n",
        "# title/contentÎßå Î∞òÌôòÌïòÎèÑÎ°ù Ï∂ïÏÜå\n",
        "class GenerateResponse(BaseModel):\n",
        "    title: str\n",
        "    content: str\n",
        "    pages: list[str]\n",
        "    page_count: int\n",
        "    contents: str\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def _startup():\n",
        "    global STORY\n",
        "    model_cfg = ModelConfig()\n",
        "    gen_cfg = GenConfig()\n",
        "    prompt_cfg = PromptConfig()\n",
        "    STORY = StoryTeller(model_cfg, gen_cfg, prompt_cfg)\n",
        "    STORY.load()\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\n",
        "        \"status\": \"ok\",\n",
        "        \"model_loaded\": STORY is not None and STORY.model is not None,\n",
        "        \"gpu\": _gpu_stats(\"Ìó¨Ïä§Ï≤¥ÌÅ¨\"),\n",
        "    }\n",
        "\n",
        "@app.post(\"/generate\", response_model=GenerateResponse)\n",
        "def generate(req: GenerateRequest):\n",
        "    assert STORY is not None, \"Server not initialized\"\n",
        "    # ÎèôÏ†Å ÌååÎùºÎØ∏ÌÑ∞ Î∞òÏòÅ\n",
        "    if req.max_new_tokens is not None: STORY.gen_cfg.max_new_tokens = req.max_new_tokens\n",
        "    if req.temperature is not None: STORY.gen_cfg.temperature = req.temperature\n",
        "    if req.top_p is not None: STORY.gen_cfg.top_p = req.top_p\n",
        "    if req.repetition_penalty is not None: STORY.gen_cfg.repetition_penalty = req.repetition_penalty\n",
        "    if req.style_override is not None: STORY.prompt_cfg.style_override = req.style_override\n",
        "    if req.length_hint is not None: STORY.prompt_cfg.length_hint = req.length_hint\n",
        "\n",
        "    prompt = STORY.build_prompt(name=req.name, age=req.age, genre=req.genre)\n",
        "    result = STORY.generate(prompt)\n",
        "\n",
        "    # ÏÉùÏÑ± ÌÖçÏä§Ìä∏ÏóêÏÑú Ï†úÎ™©/ÎÇ¥Ïö©Îßå Ï∂îÏ∂ú\n",
        "    title, content = _split_title_content(result[\"text\"])\n",
        "    pages = _paginate_content(content, sentences_per_page=3, max_chars=600)\n",
        "\n",
        "    # ÌéòÏù¥ÏßÄÎ≥Ñ \\n Íµ¨Î∂Ñ\n",
        "    contents = \"\\n\\n--- Page Break ---\\n\\n\".join(pages)\n",
        "\n",
        "    return GenerateResponse(\n",
        "        title=title,\n",
        "        content=content,\n",
        "        pages=pages,\n",
        "        page_count=len(pages),\n",
        "        contents=contents\n",
        "    )\n",
        "\n",
        "# run\n",
        "PORT = int(os.environ.get(\"PORT\", 8001))\n",
        "\n",
        "def is_port_open(port: int) -> bool:\n",
        "    with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
        "        s.settimeout(0.2)\n",
        "        return s.connect_ex((\"127.0.0.1\", port)) == 0\n",
        "\n",
        "_server_thread = None\n",
        "def run_uvicorn_background(app, host=\"0.0.0.0\", port=PORT, log_level=\"info\"):\n",
        "    global _server_thread\n",
        "    if _server_thread and _server_thread.is_alive():\n",
        "        return\n",
        "    def _target():\n",
        "        nest_asyncio.apply()\n",
        "        uvicorn.run(app, host=host, port=port, log_level=log_level)\n",
        "    _server_thread = threading.Thread(target=_target, daemon=True)\n",
        "    _server_thread.start()\n",
        "    for _ in range(80):\n",
        "        if is_port_open(port):\n",
        "            print(f\"[OK] Uvicorn running on http://127.0.0.1:{port}\")\n",
        "            return\n",
        "        time.sleep(0.25)\n",
        "    raise RuntimeError(\"Uvicorn not started in time\")\n",
        "\n",
        "run_uvicorn_background(app)\n",
        "\n",
        "# Cloudflare\n",
        "try:\n",
        "    from pycloudflared import try_cloudflare\n",
        "    public_url_cf = try_cloudflare(port=PORT)\n",
        "    print(\"üåê Cloudflare URL:\", public_url_cf)\n",
        "    print(\"  - GET :\", f\"{public_url_cf}/health\")\n",
        "    print(\"  - POST:\", f\"{public_url_cf}/generate\")\n",
        "except Exception as e:\n",
        "    print(\"[warn] Cloudflare tunnel skipped:\", repr(e))\n",
        "\n",
        "# ngrok\n",
        "USE_NGROK = False\n",
        "if USE_NGROK:\n",
        "    try:\n",
        "        from pyngrok import ngrok\n",
        "        REAL_NGROK_TOKEN = os.getenv(\"NGROK_TOKEN\", \"\")\n",
        "        if REAL_NGROK_TOKEN:\n",
        "            ngrok.set_auth_token(REAL_NGROK_TOKEN)\n",
        "            public_url_ng = ngrok.connect(addr=PORT, proto=\"http\")\n",
        "            print(\" ngrok URL    :\", public_url_ng.public_url)\n",
        "        else:\n",
        "            print(\"[warn] NGROK_TOKEN ÎπÑÏñ¥ÏûàÏùå, ngrok ÏÉùÎûµ\")\n",
        "    except Exception as e:\n",
        "        print(\"[warn] ngrok skipped:\", repr(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "collapsed": true,
        "id": "48JXpwwnKazn",
        "outputId": "49b01300-43b6-4f1b-a409-1003193d57a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading merged model from HF Hub...\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "There was a specific connection error when trying to load kkuriyoon/QLoRA-ax4-StoryTeller:\n401 Client Error: Unauthorized for url: https://huggingface.co/kkuriyoon/QLoRA-ax4-StoryTeller/resolve/main/config.json (Request ID: Root=1-68a1b641-32ab4df7665299880c3e2d7d;13872f0d-e684-4323-a928-a4c87e2689f0)\n\nInvalid credentials in Authorization header",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/kkuriyoon/QLoRA-ax4-StoryTeller/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1011\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1657\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1659\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1547\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1464\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m429\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/kkuriyoon/QLoRA-ax4-StoryTeller/resolve/main/config.json (Request ID: Root=1-68a1b641-32ab4df7665299880c3e2d7d;13872f0d-e684-4323-a928-a4c87e2689f0)\n\nInvalid credentials in Authorization header",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2370454394.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_sec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREPO_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_hf_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ÎÇòÎ¶∞\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ÌåêÌÉÄÏßÄ\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2370454394.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(repo_id, token)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2370454394.py\u001b[0m in \u001b[0;36m_try_load\u001b[0;34m(token_arg)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         tok = AutoTokenizer.from_pretrained(\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m   1070\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    706\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    709\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_connection_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{e}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Any other Exception type should now be re-raised, in order to provide helpful error messages and break the execution flow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;31m# (EntryNotFoundError will be treated outside this block and correctly re-raised if needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: There was a specific connection error when trying to load kkuriyoon/QLoRA-ax4-StoryTeller:\n401 Client Error: Unauthorized for url: https://huggingface.co/kkuriyoon/QLoRA-ax4-StoryTeller/resolve/main/config.json (Request ID: Root=1-68a1b641-32ab4df7665299880c3e2d7d;13872f0d-e684-4323-a928-a4c87e2689f0)\n\nInvalid credentials in Authorization header"
          ]
        }
      ],
      "source": [
        "import os, time, torch, re\n",
        "from typing import Optional, List\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "from huggingface_hub import login\n",
        "\n",
        "REPO_ID = \"kkuriyoon/QLoRA-ax4-StoryTeller\"\n",
        "my_hf_token = \" \"\n",
        "\n",
        "\n",
        "def _fmt_mb(b): return f\"{b/(1024**2):.2f} MB\"\n",
        "def _gpu_stats(tag):\n",
        "    if not torch.cuda.is_available(): return f\"[GPU] {tag}: CUDA ÎØ∏ÏÇ¨Ïö©\"\n",
        "    torch.cuda.synchronize()\n",
        "    a, r, p = torch.cuda.memory_allocated(), torch.cuda.memory_reserved(), torch.cuda.max_memory_allocated()\n",
        "    return f\"[GPU] {tag} | ÌòÑÏû¨ Ìï†Îãπ: {_fmt_mb(a)} | ÌòÑÏû¨ ÏòàÏïΩ: {_fmt_mb(r)} | ÌîºÌÅ¨ Ìï†Îãπ: {_fmt_mb(p)}\"\n",
        "\n",
        "def _select_dtype_and_device_map():\n",
        "    if torch.cuda.is_available(): return torch.float16, \"auto\"\n",
        "    elif torch.backends.mps.is_available(): return torch.float16, {\"\": \"mps\"}\n",
        "    return torch.float32, \"auto\"\n",
        "\n",
        "# Î™®Îç∏ Î°úÎìú\n",
        "def load_model(repo_id: str, token: Optional[str] = None):\n",
        "    if token:\n",
        "        try:\n",
        "            login(token=token, add_to_git_credential=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(\"Loading merged model from HF Hub...\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    def _try_load(token_arg):\n",
        "        tok = AutoTokenizer.from_pretrained(\n",
        "            repo_id,\n",
        "            use_fast=True,\n",
        "            trust_remote_code=True,\n",
        "            **({\"token\": token_arg} if token_arg else {})\n",
        "        )\n",
        "        if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
        "            tok.pad_token = tok.eos_token\n",
        "\n",
        "        torch_dtype, device_map = _select_dtype_and_device_map()\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            repo_id,\n",
        "            torch_dtype=torch_dtype,\n",
        "            device_map=device_map,\n",
        "            trust_remote_code=True,\n",
        "            low_cpu_mem_usage=False,\n",
        "            use_safetensors=True,\n",
        "            **({\"token\": token_arg} if token_arg else {})\n",
        "        )\n",
        "        return tok, model\n",
        "\n",
        "    tok, model = _try_load(token)\n",
        "    model.config.use_cache = True\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    load_sec = time.time() - t0\n",
        "    print(f\"Loaded in {load_sec:.2f}s\")\n",
        "    print(_gpu_stats(\"Î°úÎìú ÏßÅÌõÑ\"))\n",
        "    return tok, model, load_sec\n",
        "\n",
        "# prompt\n",
        "def default_gen_cfg(short: bool = False) -> GenerationConfig:\n",
        "    return GenerationConfig(\n",
        "        max_new_tokens=40 if short else 280,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.05,\n",
        "    )\n",
        "\n",
        "def build_prompt(\n",
        "    name: str,\n",
        "    age: int,\n",
        "    genre: str,\n",
        "    length_hint: str = \"8~12Î¨∏Ïû•\",\n",
        "    reading_level: str = \"Ìï¥Îãπ ÎÇòÏù¥ ÎòêÎûòÍ∞Ä Ïà†Ïà† ÏùΩÏùÑ Ïàò ÏûàÎäî ÎÇúÏù¥ÎèÑ\",\n",
        "    safety: str = \"Ìè≠Î†•/Í≥µÌè¨/ÌòêÏò§/Ïó∞Î†πÎ∂àÍ∞Ä ÏöîÏÜå Í∏àÏßÄ\",\n",
        "    style_override: Optional[str] = None,\n",
        "):\n",
        "    genre_guides = {\n",
        "        \"ÎèôÌôî\": \"Îî∞ÎúªÌïòÍ≥† Ìè¨Í∑ºÌïú ÌÜ§, ÏùºÏÉÅÏ†Å Í∞àÎì±Í≥º ÏûëÏùÄ Ìï¥Í≤∞, ÏùòÏÑ±Ïñ¥/ÏùòÌÉúÏñ¥ ÏÜåÎüâ\",\n",
        "        \"Î™®Ìóò\": \"Í≤ΩÏæåÌïú ÏßÑÌñâ, Î™©Ìëú-Ïû•Ïï†-ÏÑ±Ïû•Ïùò 3Îßâ, Í≥µÍ∞Ñ Ïù¥ÎèôÍ≥º ÏûëÏùÄ ÌÄòÏä§Ìä∏\",\n",
        "        \"ÎØ∏Ïä§ÌÑ∞Î¶¨\": \"Î∂ÄÎìúÎü¨Ïö¥ Ìò∏Í∏∞Ïã¨ Ïú†Î∞ú, ÏúÑÌóò ÏµúÏÜåÌôî, Îã®ÏÑú-Ï∂îÎ°†-Ìï¥Í≤∞Ïùò ÌùêÎ¶Ñ\",\n",
        "        \"ÌåêÌÉÄÏßÄ\": \"ÏÉÅÏÉÅÎ†• Í∞ÄÎìùÌïú ÏÑ∏Í≥ÑÍ¥Ä, ÎßàÎ≤ï/ÏÉÅÏßïÏùÑ ÏùÄÏú†Ï†ÅÏúºÎ°ú ÏÇ¨Ïö©\",\n",
        "        \"SF\": \"ÎØ∏Îûò/Í≥ºÌïô ÏöîÏÜåÎ•º ÏâΩÍ≥† ÏïàÏ†ÑÌïòÍ≤å ÏÑ§Î™Ö, Í∏∞Ïà†ÏùÄ ÏπúÍ∑ºÌïú ÎèÑÍµ¨Ï≤òÎüº\",\n",
        "        \"ÏùºÏÉÅ\": \"ÏπúÍµ¨/Í∞ÄÏ°±/ÌïôÍµê/ÎèôÎÑ§ Îì± Í≥µÍ∞ê Ìè¨Ïù∏Ìä∏ Ï§ëÏã¨Ïùò ÏÜåÏÜåÌïú ÏÇ¨Í±¥\",\n",
        "        \"ÎèôÏãú\": \"Î¶¨Îì¨/Î∞òÎ≥µ/Ïù¥ÎØ∏ÏßÄÎ•º ÏÇ¥Î¶∞ Ïö¥Ïú®, ÏßßÏùÄ ÌñâÍ≥º Î™ÖÎ£åÌïú Î©îÏãúÏßÄ\",\n",
        "    }\n",
        "    guide = genre_guides.get(genre, \"Ïû•Î•¥Ï†Å Í¥ÄÏäµÏùÑ Ïú†ÏïÑ ÏπúÌôîÏ†ÅÏúºÎ°ú ÏàúÌôîÌïòÏó¨ Î∞òÏòÅ\")\n",
        "\n",
        "    if style_override:\n",
        "        guide = style_override\n",
        "\n",
        "    return f\"\"\"ÎãπÏã†ÏùÄ ÏïÑÎèô Î¨∏Ìïô ÏûëÍ∞ÄÏù¥Ïûê Ïñ∏Ïñ¥Î∞úÎã¨ ÏΩîÏπòÏûÖÎãàÎã§. ÏïÑÎûò Ï†ïÎ≥¥Î•º Î∞òÏòÅÌïòÏó¨ {age}ÏÇ¥ ÏïÑÏù¥ '{name}'ÏóêÍ≤å Îî± ÎßûÎäî {genre} Ïû•Î•¥ Ïù¥ÏïºÍ∏∞(ÌïúÍµ≠Ïñ¥)Î•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî.\n",
        "\n",
        "[ÎèÖÏûê Ï†ïÎ≥¥]\n",
        "- Ïù¥Î¶Ñ: {name}\n",
        "- ÎÇòÏù¥: {age}ÏÑ∏\n",
        "- ÏùΩÍ∏∞ ÎÇúÏù¥ÎèÑ: {reading_level}\n",
        "\n",
        "[Ïû•Î•¥ Í∞ÄÏù¥Îìú]\n",
        "- {guide}\n",
        "\n",
        "[Ïä§ÌÜ†Î¶¨ Íµ¨ÏÑ±(Ï†úÎ°úÏÉ∑ ÏßÄÏãú)]\n",
        "1) Ï≤´ Î¨∏Ïû•ÏùÄ ÏûêÏó∞Ïä§ÎüΩÍ≤å ÏÉÅÌô©/Ïû•Î©¥/Í∞êÏ†ï Ï§ë ÌïòÎÇòÎ°ú ÏãúÏûëÌï©ÎãàÎã§. ÌäπÏ†ï Í≥†Ï†ï Î¨∏Íµ¨Î°ú ÏãúÏûëÌïòÏßÄ ÎßàÏÑ∏Ïöî.\n",
        "2) {name}Ïù¥(Í∞Ä) Í≤™Îäî ÏûëÏùÄ Ïñ¥Î†§ÏõÄ ‚Üí ÏãúÎèÑ/ÎèÑÏõÄ ‚Üí Ïä§Ïä§Î°ú(ÎòêÎäî ÏπúÍµ¨ÏôÄ Ìï®Íªò) Ìï¥Í≤∞.\n",
        "3) Ïû•Î©¥ Ï†ÑÌôòÏùÄ Í≥ºÎèÑÌïòÏßÄ ÏïäÍ≤å 2~3Ìöå Ïù¥ÎÇ¥Î°ú.\n",
        "4) ÎåÄÏÇ¨Îäî 2~4Í≥≥ÏóêÎßå ÏûêÏó∞Ïä§ÎüΩÍ≤å ÏÑûÎêò, Í≥ºÎèÑÌïú Í∞êÌÉÑÏÇ¨/Î∞òÎ≥µÏùÄ ÏßÄÏñë.\n",
        "5) ÎπÑÏú†/ÏÉÅÏßïÏùÄ ÎÇòÏù¥Ïóê ÎßûÍ≤å Ïâ¨Ïö¥ Îã®Ïñ¥Î°ú.\n",
        "\n",
        "[Í∏∏Ïù¥]\n",
        "- Î¨∏Ïû• Ïàò: {length_hint}\n",
        "\n",
        "[ÏïàÏ†Ñ/Ïú§Î¶¨]\n",
        "- {safety}\n",
        "- Ïã§Ï†ú Ïù∏Î¨º¬∑Î∏åÎûúÎìú¬∑Ï†ïÏπòÏ†Å ÏÇ¨Ïïà Ïñ∏Í∏â Í∏àÏßÄ.\n",
        "- ÌëúÏ†à Í∏àÏßÄ, Ï†ÑÍ∞úÎäî ÏÉàÎ°≠Í≤å Íµ¨ÏÑ±.\n",
        "\n",
        "[Ï∂úÎ†• ÌòïÏãù(Ï§ëÏöî)]\n",
        "- Ïù¥ÏïºÍ∏∞ Î≥∏Î¨∏\n",
        "- Í≥µÎ∞± Ìïú Ï§Ñ\n",
        "- Ï†úÎ™©: (Ïù¥ÏïºÍ∏∞Ïóê Ïñ¥Ïö∏Î¶¨Îäî ÏßßÍ≥† Ïù∏ÏÉÅÏ†ÅÏù∏ Ï±Ö Ï†úÎ™©)\n",
        "\n",
        "Ïù¥Ï†ú '{genre}' Ïû•Î•¥Ïùò Ïù¥ÏïºÍ∏∞Î•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî.\"\"\"\n",
        "\n",
        "# generate\n",
        "@torch.inference_mode()\n",
        "def generate_story(tokenizer, model, prompt, gen_cfg: Optional[GenerationConfig] = None):\n",
        "    gen_cfg = gen_cfg or default_gen_cfg(short=False)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    t0 = time.time()\n",
        "    # ÏõåÎ∞çÏóÖ\n",
        "    _ = model.generate(**{k: v[:, :4] for k, v in inputs.items()}, max_new_tokens=1, do_sample=False)\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=gen_cfg.max_new_tokens,\n",
        "        temperature=gen_cfg.temperature,\n",
        "        top_p=gen_cfg.top_p,\n",
        "        do_sample=gen_cfg.do_sample,\n",
        "        repetition_penalty=gen_cfg.repetition_penalty,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        use_cache=True,\n",
        "    )\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    gen_sec = time.time() - t0\n",
        "\n",
        "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    if text.startswith(prompt):\n",
        "        text = text[len(prompt):].lstrip()\n",
        "\n",
        "    new_tokens = out[0].shape[0] - inputs[\"input_ids\"].shape[1]\n",
        "    tokps = new_tokens / max(gen_sec, 1e-6)\n",
        "\n",
        "    gpu_summary = None\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_summary = {\n",
        "            \"alloc_now\": _fmt_mb(torch.cuda.memory_allocated()),\n",
        "            \"reserved_now\": _fmt_mb(torch.cuda.memory_reserved()),\n",
        "            \"peak_alloc\": _fmt_mb(torch.cuda.max_memory_allocated()),\n",
        "        }\n",
        "\n",
        "    return text, gen_sec, new_tokens, tokps, gpu_summary\n",
        "\n",
        "# run\n",
        "if __name__ == \"__main__\":\n",
        "    tokenizer, model, load_sec = load_model(REPO_ID, token=my_hf_token)\n",
        "\n",
        "    prompt = build_prompt(name=\"ÎÇòÎ¶∞\", age=7, genre=\"ÌåêÌÉÄÏßÄ\")\n",
        "\n",
        "    story_full, gen_sec, new_tokens, tokps, gpu_summary = generate_story(\n",
        "        tokenizer, model, prompt\n",
        "    )\n",
        "\n",
        "    print(\"\\n üìç ÎèôÌôî Ï∂úÎ†• \\n\")\n",
        "    print(story_full)\n",
        "\n",
        "    print(\"\\n üìç ÏöîÏïΩ(SUMMARY)\")\n",
        "    print(f\"- Î™®Îç∏ Î°úÎìú ÏãúÍ∞Ñ: {load_sec:.2f} s\")\n",
        "    print(f\"- ÏÉùÏÑ± ÏãúÍ∞Ñ: {gen_sec:.2f} s | ÏÉùÏÑ± ÌÜ†ÌÅ∞: {new_tokens} | ÏÜçÎèÑ: {tokps:.2f} tok/s\")\n",
        "    if gpu_summary:\n",
        "        print(f\"- GPU ÌòÑÏû¨ Ìï†Îãπ: {gpu_summary['alloc_now']}\")\n",
        "        print(f\"- GPU ÌòÑÏû¨ ÏòàÏïΩ: {gpu_summary['reserved_now']}\")\n",
        "        print(f\"- GPU ÌîºÌÅ¨ Ìï†Îãπ(Ïù¥Î≤à ÏÉùÏÑ± Í∏∞Ï§Ä): {gpu_summary['peak_alloc']}\")\n",
        "    else:\n",
        "        print(\"- GPU ÎØ∏ÏÇ¨Ïö©(CPU/MPS) ÌôòÍ≤Ω\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzc_jl-5p5XB",
        "outputId": "b769b600-f3c1-4425-fd22-8db49ca7b167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF cached token removed\n"
          ]
        }
      ],
      "source": [
        "# ÌôòÍ≤ΩÎ≥ÄÏàòÏóê ÌÜ†ÌÅ∞Ïù¥ Ïû°ÌòÄ ÏûàÏúºÎ©¥ ÎπÑÏö∞Í∏∞\n",
        "import os\n",
        "for k in [\"HF_TOKEN\", \"HUGGINGFACE_HUB_TOKEN\", \"HUGGINGFACEHUB_API_TOKEN\", \"HF_HOME\"]:\n",
        "    if k in os.environ: os.environ.pop(k)\n",
        "\n",
        "# huggingface_hub Ï∫êÏãúÎêú ÌÜ†ÌÅ∞ Ï†úÍ±∞\n",
        "from huggingface_hub import HfFolder\n",
        "try:\n",
        "    HfFolder.delete_token()\n",
        "    print(\"HF cached token removed\")\n",
        "except Exception as e:\n",
        "    print(\"skip:\", e)\n",
        "\n",
        "# netrcÍ∞Ä ÏûàÏúºÎ©¥ requestsÍ∞Ä ÏûêÎèô Î°úÍ∑∏Ïù∏ ‚Üí Ïù¥Î¶Ñ Î≥ÄÍ≤Ω\n",
        "import os, pathlib, shutil\n",
        "p = pathlib.Path.home()/\".netrc\"\n",
        "if p.exists():\n",
        "    shutil.move(str(p), str(p)+\".bak\")\n",
        "    print(\"~/.netrc -> ~/.netrc.bak\")\n",
        "\n",
        "# (D) git-credentialsÏóê huggingface.co Ï§ÑÏù¥ ÏûàÏúºÎ©¥ ÏÇ≠Ï†ú Í∂åÏû•\n",
        "!if [ -f ~/.git-credentials ]; then sed -n '1,200p' ~/.git-credentials; fi"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "078c5695779d45dc98dcf384b41e554f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08bb4c8c0824429d818681def026fedd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64c396a91310465ea0e6c097d534142f",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f103dd8a5cf1403e919e3e37b4cef01d",
            "value": 3
          }
        },
        "10e62def10134146af20a3f5eab93229": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1226f8c00dd244d7901ec4182ecf2484": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fecd95216d343769adb1fa41349c3c3",
              "IPY_MODEL_08bb4c8c0824429d818681def026fedd",
              "IPY_MODEL_3c3e441c019f401b894b38044776e3d4"
            ],
            "layout": "IPY_MODEL_57acd1639b0d4fad97f8b734978e7edc"
          }
        },
        "1b8de40e25744e4595b929162ea6ab6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fecd95216d343769adb1fa41349c3c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10e62def10134146af20a3f5eab93229",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1b8de40e25744e4595b929162ea6ab6d",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "2b62d9a6766a4e70b92d5b1d6a2e3845": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c3e441c019f401b894b38044776e3d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_078c5695779d45dc98dcf384b41e554f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2b62d9a6766a4e70b92d5b1d6a2e3845",
            "value": "‚Äá3/3‚Äá[00:04&lt;00:00,‚Äá‚Äá1.36s/it]"
          }
        },
        "57acd1639b0d4fad97f8b734978e7edc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64c396a91310465ea0e6c097d534142f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f103dd8a5cf1403e919e3e37b4cef01d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
